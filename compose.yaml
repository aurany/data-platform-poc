version: "3"

services:

  cassandra:
    #image: debezium/example-cassandra:${DEBEZIUM_VERSION}
    container_name: cassandra
    hostname: cassandra
    networks:
      platform:
    build:
      dockerfile: Dockerfile
      context: cassandra
      args:
        - DEBEZIUM_VERSION=${DEBEZIUM_VERSION}
        - TZ=Europe/Stockholm
    ports:
      - 9042:9042
    environment:
      - CQLENG_ALLOW_SCHEMA_MANAGEMENT=1
    depends_on:
      - kafka
    healthcheck:
      test: ["CMD-SHELL", "[ $$(nodetool statusgossip) = running ]"]
      interval: 30s
      timeout: 10s
      retries: 5

  kafka:
    image: confluentinc/cp-kafka:7.5.3
    hostname: kafka
    container_name: kafka
    networks:
      platform:
    ports:
      - "9092:9092"
      - "9101:9101"
    environment:
      TZ: 'Europe/Stockholm'
      KAFKA_NODE_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:9092'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka:9094'
      KAFKA_LISTENERS: 'PLAINTEXT://:9092,CONTROLLER://:9094'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'
      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'

  connect:
    build:
      dockerfile: Dockerfile
      context: connect
      args:
        - DEBEZIUM_VERSION=${DEBEZIUM_VERSION}
    #image: confluentinc/cp-kafka-connect-base:7.5.3
    hostname: connect
    container_name: connect
    networks:
      platform:
    depends_on:
      - kafka
      - minio
    ports:
      - 8083:8083
    environment:
      TZ: 'Europe/Stockholm'
      CONNECT_BOOTSTRAP_SERVERS: 'kafka:9092'
      CONNECT_REST_ADVERTISED_HOST_NAME: "connect"
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: connect-cluster-group
      CONNECT_CONFIG_STORAGE_TOPIC: _kafka-connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _kafka-connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _kafka-connect-status
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_PLUGIN_PATH: '/usr/share/java,/usr/share/confluent-hub-components/,/connectors/'
      AWS_ACCESS_KEY_ID: "admin"
      AWS_SECRET_ACCESS_KEY: "password"
      AWS_REGION: "us-east-1"
    volumes:
      - ./connect/start.sh:/scripts/start.sh
    command: /bin/bash /scripts/start.sh

  kafka-ui:
    image: 'provectuslabs/kafka-ui:latest'
    container_name: kafka-ui
    hostname: kafka-ui
    networks:
      platform:
    ports:
      - "8880:8080"
    depends_on:
      - kafka
      #- schema-registry
    environment:
      TZ: 'Europe/Stockholm'
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      #KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
      #KAFKA_CLUSTERS_0_METRICS_PORT: 9997

  spark:
    #image: tabulario/spark-iceberg
    build:
      dockerfile: Dockerfile
      context: spark
    hostname: spark
    container_name: spark
    networks:
      platform:
    depends_on:
      - rest
      - minio
    environment:
      - TZ=Europe/Stockholm
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
    volumes:
      - ./spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./spark/query.sql:/home/iceberg/query.sql
      - ./warehouse:/home/iceberg/warehouse
    ports:
      - 8888:8888
      - 8080:8080
      - 10000:10000
      - 10001:10001

  rest:
    image: tabulario/iceberg-rest
    hostname: rest
    container_name: rest
    networks:
      platform:
    ports:
      - 8181:8181
    environment:
      - TZ=Europe/Stockholm
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - CATALOG_WAREHOUSE=s3://warehouse/
      - CATALOG_IO__IMPL=org.apache.iceberg.aws.s3.S3FileIO
      - CATALOG_S3_ENDPOINT=http://minio:9000

  minio:
    image: minio/minio
    hostname: minio
    container_name: minio
    environment:
      - TZ=Europe/Stockholm
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
      - MINIO_DOMAIN=minio
    networks:
      platform:
        aliases:
          - warehouse.minio
    ports:
      - 9001:9001
      - 9000:9000
    command: ["server", "/data", "--console-address", ":9001"]

  mc:
    image: minio/mc
    hostname: mc
    container_name: mc
    depends_on:
      - minio
    networks:
      platform:
    environment:
      - TZ=Europe/Stockholm
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc config host add minio http://minio:9000 admin password) do echo '...waiting...' && sleep 1; done;
      /usr/bin/mc rm -r --force minio/warehouse;
      /usr/bin/mc mb minio/warehouse;
      /usr/bin/mc policy set public minio/warehouse;
      tail -f /dev/null
      "

  data-generator:
    build:
      dockerfile: Dockerfile
      context: app
    hostname: data-generator
    container_name: data-generator
    networks:
      platform:
    depends_on:
      cassandra:
        condition: service_healthy
      connect:
        condition: service_healthy
    environment:
      - TZ=Europe/Stockholm


networks:
  platform:

  # schema-registry:
  #   image: confluentinc/cp-schema-registry:7.5.3
  #   hostname: schema-registry
  #   container_name: schema-registry
  #   depends_on:
  #     - kafka
  #   ports:
  #     - "8081:8081"
  #   environment:
  #     SCHEMA_REGISTRY_HOST_NAME: schema-registry
  #     SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'kafka:9092'
  #     SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081

  # spark-master:
  #   container_name: spark-master
  #   hostname: spark-master
  #   build:
  #     dockerfile: Dockerfile
  #     context: spark
  #   depends_on:
  #     - minio
  #   environment:
  #     - SPARK_MODE=master
  #     - SPARK_LOCAL_IP=spark-master
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_SSL_ENABLED=no
  #   ports:
  #     - "7077:7077"
  #     - "8088:8080"

  # spark-worker:
  #   build:
  #     dockerfile: Dockerfile
  #     context: spark
  #   container_name: spark-worker
  #   hostname: spark-worker
  #   depends_on:
  #     - spark-master
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark-master:7077
  #     - SPARK_WORKER_MEMORY=1G
  #     - SPARK_WORKER_CORES=1
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_SSL_ENABLED=no

  # app:
  #   build:
  #       dockerfile: Dockerfile
  #       context: spark
  #   container_name: app
  #   hostname: app
  #   depends_on:
  #     - spark-master
  #   volumes:
  #     - ./app.py:/app.py